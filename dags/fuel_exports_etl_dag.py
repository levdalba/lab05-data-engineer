"""
Fuel Exports ETL DAG

This DAG monitors for new parquet files generated by the data generator
and loads them into PostgreSQL database every minute.
"""

from datetime import datetime, timedelta
import os
import glob
import logging
from typing import List
import sys

# Add plugins directory to Python path
sys.path.append("/opt/airflow/plugins")

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.postgres.operators.postgres import PostgresOperator
from airflow.providers.postgres.hooks.postgres import PostgresHook
from airflow.models import Variable

# Import our custom data processing utilities
from data_processor import DataProcessor, FileTracker, PostgreSQLLoader

# Default arguments for the DAG
default_args = {
    "owner": "data-engineer",
    "depends_on_past": False,
    "start_date": datetime(2024, 1, 1),
    "email_on_failure": False,
    "email_on_retry": False,
    "retries": 1,
    "retry_delay": timedelta(minutes=5),
}

# DAG definition
dag = DAG(
    "fuel_exports_etl",
    default_args=default_args,
    description="ETL pipeline for fuel exports data",
    schedule_interval=timedelta(minutes=1),  # Run every minute
    catchup=False,
    max_active_runs=1,
    tags=["etl", "fuel-exports", "postgresql"],
)

# Configuration
DATA_DIR = Variable.get("fuel_exports_data_dir", default_var="/opt/airflow/data")
POSTGRES_CONN_ID = Variable.get(
    "fuel_exports_postgres_conn", default_var="postgres_default"
)
PROCESSED_FILES_TABLE = "processed_files"
FUEL_EXPORTS_TABLE = "fuel_exports"


def get_new_files(**context) -> List[str]:
    """
    Scan the data directory for new parquet files that haven't been processed yet.

    Returns:
        List of new file paths to process
    """
    logging.info(f"Scanning directory: {DATA_DIR}")

    # Get all parquet files in the data directory
    pattern = os.path.join(DATA_DIR, "fuel_export_*.parquet")
    all_files = glob.glob(pattern)

    if not all_files:
        logging.info("No parquet files found")
        return []

    # Get list of already processed files from database
    postgres_hook = PostgresHook(postgres_conn_id=POSTGRES_CONN_ID)
    file_tracker = FileTracker(postgres_hook)
    processed_files = file_tracker.get_processed_files()

    # Filter out already processed files
    new_files = []
    for file_path in all_files:
        filename = os.path.basename(file_path)
        if filename not in processed_files:
            new_files.append(file_path)

    logging.info(f"Found {len(new_files)} new files to process: {new_files}")
    return new_files


def process_parquet_files(**context) -> None:
    """
    Process new parquet files and load them into PostgreSQL.
    """
    # Get new files from previous task
    ti = context["ti"]
    new_files = ti.xcom_pull(task_ids="scan_for_new_files")

    if not new_files:
        logging.info("No new files to process")
        return

    postgres_hook = PostgresHook(postgres_conn_id=POSTGRES_CONN_ID)
    file_tracker = FileTracker(postgres_hook)
    postgres_loader = PostgreSQLLoader(postgres_hook, FUEL_EXPORTS_TABLE)

    for file_path in new_files:
        try:
            logging.info(f"Processing file: {file_path}")

            # Read and process the parquet file
            df = DataProcessor.read_parquet_file(file_path)
            df_transformed = DataProcessor.transform_for_postgres(df)

            # Validate the data
            if not DataProcessor.validate_data(df_transformed):
                logging.error(f"Data validation failed for {file_path}")
                continue

            # Insert data into PostgreSQL
            postgres_loader.insert_dataframe(df_transformed)

            # Mark file as processed
            file_tracker.mark_file_processed(os.path.basename(file_path))

            logging.info(
                f"Successfully processed {file_path} with {len(df_transformed)} records"
            )

        except Exception as e:
            logging.error(f"Error processing file {file_path}: {e}")
            raise


# Task definitions

# Task 1: Create tables if they don't exist
create_tables_task = PostgresOperator(
    task_id="create_tables",
    postgres_conn_id=POSTGRES_CONN_ID,
    sql="sql/create_tables.sql",
    dag=dag,
)

# Task 2: Scan for new files
scan_files_task = PythonOperator(
    task_id="scan_for_new_files",
    python_callable=get_new_files,
    dag=dag,
)

# Task 3: Process new files
process_files_task = PythonOperator(
    task_id="process_files",
    python_callable=process_parquet_files,
    dag=dag,
)

# Define task dependencies
create_tables_task >> scan_files_task >> process_files_task
