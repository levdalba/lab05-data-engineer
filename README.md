# Fuel Exports ETL with Apache Airflow

This project implements an ETL (Extract, Transform, Load) pipeline using Apache Airflow to process fuel export data generated by a Python script and load it into a PostgreSQL database.

## Overview

The pipeline consists of:

1. **Data Generator**: A Python script that continuously generates parquet files with synthetic fuel transaction data
2. **Airflow DAG**: Monitors for new files every minute and processes them
3. **PostgreSQL Database**: Stores the processed fuel export data

## Project Structure

```
# Fuel Exports ETL Pipeline with Apache Airflow

A complete ETL (Extract, Transform, Load) pipeline built with Apache Airflow that processes synthetic space fuel station transaction data from Parquet files into a PostgreSQL database.

## 🚀 Features

- **Real-time Data Processing**: Monitors for new Parquet files every minute
- **Robust ETL Pipeline**: Extracts, transforms, and loads complex data structures
- **Idempotent Processing**: Files are processed only once, preventing duplicates
- **Complex Data Handling**: Processes nested objects, arrays, and various data types
- **Production Ready**: Containerized with Docker for easy deployment
- **Comprehensive Monitoring**: Full observability through Airflow UI
- **Error Handling**: Robust error handling with detailed logging

## 📁 Project Structure

```

lab05-data-engineer/
├── dags/ # Airflow DAG definitions
│ └── fuel_exports_etl_dag.py # Main ETL DAG
├── plugins/ # Custom Airflow plugins
│ └── data_processor.py # Data processing utilities
├── sql/ # Database schema
│ └── create_tables.sql # PostgreSQL table definitions
├── config/ # Configuration files
│ └── .env.template # Environment variables template
├── data/ # Data directory (auto-generated)
├── docker-compose.yml # Docker services configuration
├── Dockerfile # Custom Airflow image
├── requirements.txt # Python dependencies
├── generate_fuel_exports.py # Data generator script
├── setup.sh # Automated setup script
└── README.md # This file

````

## 🛠️ Quick Start

### Prerequisites

- Docker Desktop installed and running
- Git (for cloning)
- Python 3.8+ (for data generator)

### 1. Setup

Run the automated setup script:

```bash
./setup.sh
````

This will:

-   Start all services (Airflow, PostgreSQL, Redis)
-   Initialize the Airflow database
-   Create necessary directories and permissions

### 2. Access Airflow

Open your browser and navigate to: http://localhost:8080

-   **Username**: `admin`
-   **Password**: `admin`

### 3. Enable the DAG

1. In the Airflow UI, find the `fuel_exports_etl` DAG
2. Toggle it to "ON" to enable it
3. The DAG will start monitoring for new files every minute

### 4. Generate Data

Install Python dependencies and start the data generator:

```bash
pip install faker pyarrow pandas
python generate_fuel_exports.py --rows-per-file 300 --period-seconds 60
```

This will generate a new Parquet file every minute with 300 synthetic records.

## 🔧 Manual Setup (Alternative)

If you prefer manual setup:

### 1. Environment Setup

```bash
# Copy environment template
cp config/.env.template .env

# Create directories
mkdir -p data logs

# Set permissions
sudo chown -R 50000:0 logs/ data/
```

### 2. Start Services

```bash
# Initialize Airflow
docker-compose up airflow-init

# Start all services
docker-compose up -d

# Check service status
docker-compose ps
```

### 3. Database Setup

```bash
# Create tables
docker exec -i lab05-data-engineer-postgres-1 psql -U airflow -d airflow < sql/create_tables.sql
```

## 📊 Data Schema

### Source Data (Parquet)

The data generator creates files with the following schema:

```
transaction_id: string          # Unique transaction identifier
station_id: int32              # Space station ID (1000-9999)
dock: struct                   # Docking information
  ├── bay: int16              #   Docking bay number
  └── level: string           #   Docking level (A-H)
ship_name: string              # Spaceship name
franchise: string              # Ship franchise/universe
captain_name: string           # Captain name
species: string                # Captain species
fuel_type: string              # Type of fuel purchased
fuel_units: float32            # Amount of fuel
price_per_unit: decimal(8,2)   # Price per fuel unit
total_cost: decimal(12,2)      # Total transaction cost
services: list<string>         # Additional services purchased
is_emergency: boolean          # Emergency refueling flag
visited_at: timestamp(ns, UTC) # Transaction timestamp
arrival_date: date32           # Arrival date
coords_x: float64              # X coordinate
coords_y: float64              # Y coordinate
```

### Target Database (PostgreSQL)

The ETL pipeline transforms and loads data into:

```sql
CREATE TABLE fuel_exports (
    id SERIAL PRIMARY KEY,
    transaction_id VARCHAR(100) UNIQUE NOT NULL,
    station_id INTEGER NOT NULL,
    dock_bay SMALLINT,
    dock_level VARCHAR(10),
    ship_name VARCHAR(100),
    franchise VARCHAR(50),
    captain_name VARCHAR(100),
    species VARCHAR(50),
    fuel_type VARCHAR(50),
    fuel_units REAL,
    price_per_unit DECIMAL(8,2),
    total_cost DECIMAL(12,2),
    services TEXT,                    -- Comma-separated services
    is_emergency BOOLEAN,
    visited_at TIMESTAMP WITH TIME ZONE,
    arrival_date DATE,
    coords_x DOUBLE PRECISION,
    coords_y DOUBLE PRECISION,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);
```

## 🔄 ETL Pipeline Details

### DAG Configuration

-   **Schedule**: Every minute (`timedelta(minutes=1)`)
-   **Catchup**: Disabled (only processes new data)
-   **Max Active Runs**: 1 (prevents overlapping runs)

### Pipeline Steps

1. **Create Tables**: Ensures database schema exists
2. **Scan for Files**: Identifies new Parquet files in the data directory
3. **Process Files**:
    - Reads Parquet files
    - Transforms complex data structures
    - Validates data integrity
    - Loads into PostgreSQL
    - Marks files as processed

### Data Transformations

-   **Struct Flattening**: Converts `dock` struct to separate columns
-   **Array Serialization**: Converts `services` array to comma-separated string
-   **Type Conversion**: Handles decimal, timestamp, and boolean types
-   **Data Validation**: Ensures data quality before insertion

## 🐳 Docker Services

The pipeline uses the following services:

-   **airflow-webserver**: Web UI (port 8080)
-   **airflow-scheduler**: Task scheduler
-   **airflow-worker**: Task executor
-   **airflow-triggerer**: Event triggers
-   **postgres**: Database (port 5432)
-   **redis**: Message broker

## 📈 Monitoring & Observability

### Airflow UI (http://localhost:8080)

-   **DAGs**: View and manage pipeline
-   **Task Instances**: Monitor individual task execution
-   **Logs**: Detailed execution logs
-   **Variables**: Configuration management
-   **Connections**: Database connections

### Database Access

Connect to PostgreSQL:

```bash
# Via Docker
docker exec -it lab05-data-engineer-postgres-1 psql -U airflow -d airflow

# Via local client
psql -h localhost -p 5432 -U airflow -d airflow
```

### Useful Queries

```sql
-- Check processed files
SELECT * FROM processed_files ORDER BY processed_at DESC LIMIT 10;

-- View recent transactions
SELECT * FROM fuel_exports ORDER BY created_at DESC LIMIT 10;

-- Count records by fuel type
SELECT fuel_type, COUNT(*) as count
FROM fuel_exports
GROUP BY fuel_type
ORDER BY count DESC;

-- Emergency transactions
SELECT * FROM fuel_exports WHERE is_emergency = true;
```

## 🛠️ Development

### Custom Components

-   **DataProcessor**: Handles Parquet reading and transformation
-   **FileTracker**: Manages processed file tracking
-   **PostgreSQLLoader**: Optimized database loading

### Adding Features

1. **New Transformations**: Modify `plugins/data_processor.py`
2. **Schema Changes**: Update `sql/create_tables.sql`
3. **DAG Logic**: Modify `dags/fuel_exports_etl_dag.py`

### Testing

```bash
# Test data generator
python generate_fuel_exports.py --rows-per-file 10 --period-seconds 5

# Test database connection
docker exec -i lab05-data-engineer-postgres-1 psql -U airflow -d airflow -c "SELECT COUNT(*) FROM fuel_exports;"

# View Airflow logs
docker-compose logs airflow-scheduler
```

## 🔧 Configuration

### Environment Variables

Key configuration in `.env`:

```bash
# Database
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=fuel_exports
POSTGRES_USER=airflow
POSTGRES_PASSWORD=airflow

# Airflow
AIRFLOW__CORE__EXECUTOR=LocalExecutor
AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@localhost:5432/airflow

# ETL
FUEL_EXPORTS_DATA_DIR=/opt/airflow/data
```

### Airflow Variables

Set in Airflow UI under Admin > Variables:

-   `fuel_exports_data_dir`: Data directory path
-   `fuel_exports_postgres_conn`: PostgreSQL connection ID

## 🚨 Troubleshooting

### Common Issues

1. **Permission Denied on Data Directory**

    ```bash
    sudo chown -R $(whoami):staff data/
    ```

2. **Docker Not Found**

    ```bash
    export PATH="/usr/local/bin:$PATH"
    ```

3. **Database Connection Error**

    - Ensure PostgreSQL container is running
    - Check connection settings in Airflow

4. **DAG Not Visible**
    - Check DAG syntax: `python dags/fuel_exports_etl_dag.py`
    - Restart scheduler: `docker-compose restart airflow-scheduler`

### Logs

```bash
# View all logs
docker-compose logs

# Specific service logs
docker-compose logs airflow-scheduler
docker-compose logs postgres

# Follow logs
docker-compose logs -f airflow-webserver
```

## 🛑 Stopping Services

```bash
# Stop all services
docker-compose down

# Stop and remove volumes (WARNING: deletes all data)
docker-compose down -v
```

## 📝 Data Sample

Example of generated data:

```json
{
    "transaction_id": "20250922050419123456-abc123def4",
    "station_id": 7834,
    "dock": { "bay": 42, "level": "C" },
    "ship_name": "Millennium Falcon",
    "franchise": "Star Wars",
    "captain_name": "Luke Martinez",
    "species": "Human",
    "fuel_type": "HyperMatter",
    "fuel_units": 2847.65,
    "price_per_unit": 127.89,
    "total_cost": 364212.47,
    "services": ["hull patch", "oxygen refill"],
    "is_emergency": false,
    "visited_at": "2025-09-22T05:04:19.123456Z",
    "arrival_date": "2025-09-22",
    "coords_x": -7834.123,
    "coords_y": 2891.456
}
```

## 🎯 Next Steps

1. **Enable the DAG** in Airflow UI
2. **Start the data generator** to begin processing
3. **Monitor the pipeline** through the Airflow interface
4. **Query the database** to analyze the processed data
5. **Scale as needed** by adjusting Docker resources

## 📚 Additional Resources

-   [Apache Airflow Documentation](https://airflow.apache.org/docs/)
-   [PostgreSQL Documentation](https://www.postgresql.org/docs/)
-   [Docker Compose Documentation](https://docs.docker.com/compose/)
-   [PyArrow Documentation](https://arrow.apache.org/docs/python/)

---

**🚀 Happy Data Engineering!**

This pipeline demonstrates a production-ready ETL solution using modern data engineering tools and best practices.
├── dags/
│ └── fuel_exports_etl_dag.py # Main Airflow DAG
├── plugins/
│ └── data_processor.py # Data processing utilities
├── sql/
│ └── create_tables.sql # Database schema
├── config/
│ └── .env.template # Environment configuration template
├── data/ # Directory for generated parquet files
├── generate_fuel_exports.py # Data generator script
├── requirements.txt # Python dependencies
├── docker-compose.yml # Docker services configuration
├── Dockerfile # Airflow container configuration
└── README.md # This file

````

## Features

### Data Generator
- Generates realistic fuel station transaction data
- Creates parquet files with complex data types (structs, arrays)
- Configurable generation frequency and file size
- Includes data for interspace fuel stations with sci-fi themed content

### ETL Pipeline
- **Extract**: Monitors data directory for new parquet files
- **Transform**: Converts complex parquet data types to PostgreSQL-compatible formats
- **Load**: Inserts data into PostgreSQL with proper error handling
- **Tracking**: Maintains record of processed files to avoid duplicates

### Key Features
- **Idempotent Processing**: Files are processed only once
- **Error Handling**: Robust error handling with logging
- **Data Validation**: Validates data before insertion
- **Scalable Architecture**: Uses modular design for easy maintenance

## Setup Instructions

### Prerequisites
- Docker and Docker Compose
- Python 3.11+
- At least 4GB RAM

### Quick Start

1. **Clone the repository**:
   ```bash
   git clone <repository-url>
   cd lab05-data-engineer
````

2. **Start the services**:

    ```bash
    # Initialize Airflow
    docker-compose up airflow-init

    # Start all services
    docker-compose up -d
    ```

3. **Access Airflow Web UI**:

    - URL: http://localhost:8080
    - Username: `admin`
    - Password: `admin`

4. **Configure PostgreSQL connection in Airflow**:

    - Go to Admin → Connections
    - Create a new connection with ID: `postgres_default`
    - Set connection details:
        - Host: `postgres`
        - Database: `airflow`
        - Username: `airflow`
        - Password: `airflow`
        - Port: `5432`

5. **Start the data generator**:

    ```bash
    # Install dependencies for the generator
    pip install faker pyarrow pandas

    # Start generating data
    python generate_fuel_exports.py --rows-per-file 300 --period-seconds 60
    ```

6. **Enable the DAG**:
    - In Airflow UI, toggle the `fuel_exports_etl` DAG to ON
    - The DAG will start processing files every minute

### Manual Setup (Without Docker)

1. **Install dependencies**:

    ```bash
    pip install -r requirements.txt
    ```

2. **Set up PostgreSQL**:

    ```bash
    # Create database and user
    createdb fuel_exports
    psql fuel_exports < sql/create_tables.sql
    ```

3. **Configure Airflow**:

    ```bash
    export AIRFLOW_HOME=$(pwd)
    airflow db init
    airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com
    ```

4. **Start Airflow**:

    ```bash
    # Start scheduler
    airflow scheduler &

    # Start webserver
    airflow webserver --port 8080
    ```

## Configuration

### Environment Variables

Copy `config/.env.template` to `.env` and customize:

```bash
# PostgreSQL Configuration
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=fuel_exports
POSTGRES_USER=airflow
POSTGRES_PASSWORD=airflow

# Airflow Configuration
FUEL_EXPORTS_DATA_DIR=/opt/airflow/data
FUEL_EXPORTS_POSTGRES_CONN=postgres_default
```

### Data Generator Options

```bash
python generate_fuel_exports.py --help

Options:
  --rows-per-file INTEGER    Number of rows per file (default: 300)
  --period-seconds INTEGER   Seconds between files (default: 60)
  --out-dir TEXT            Output directory (default: "data")
```

## Database Schema

### fuel_exports table

```sql
CREATE TABLE fuel_exports (
    id SERIAL PRIMARY KEY,
    transaction_id VARCHAR(100) UNIQUE NOT NULL,
    station_id INTEGER NOT NULL,
    dock_bay SMALLINT,
    dock_level VARCHAR(10),
    ship_name VARCHAR(100),
    franchise VARCHAR(50),
    captain_name VARCHAR(100),
    species VARCHAR(50),
    fuel_type VARCHAR(50),
    fuel_units REAL,
    price_per_unit DECIMAL(8,2),
    total_cost DECIMAL(12,2),
    services TEXT,
    is_emergency BOOLEAN,
    visited_at TIMESTAMP WITH TIME ZONE,
    arrival_date DATE,
    coords_x DOUBLE PRECISION,
    coords_y DOUBLE PRECISION,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);
```

### processed_files table

```sql
CREATE TABLE processed_files (
    id SERIAL PRIMARY KEY,
    filename VARCHAR(255) UNIQUE NOT NULL,
    processed_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);
```

## DAG Details

### fuel_exports_etl DAG

-   **Schedule**: Every minute (`timedelta(minutes=1)`)
-   **Start Date**: 2024-01-01
-   **Catchup**: Disabled
-   **Max Active Runs**: 1

### Tasks

1. **create_tables**: Creates database tables if they don't exist
2. **scan_for_new_files**: Scans data directory for unprocessed parquet files
3. **process_files**: Reads, transforms, and loads data into PostgreSQL

### Task Dependencies

```
create_tables >> scan_for_new_files >> process_files
```

## Data Processing Pipeline

### Extract

-   Scans the configured data directory for `fuel_export_*.parquet` files
-   Filters out already processed files using the `processed_files` table

### Transform

-   Converts parquet struct types to separate columns (dock → dock_bay, dock_level)
-   Converts arrays to comma-separated strings (services)
-   Handles datetime and decimal type conversions
-   Validates data integrity

### Load

-   Inserts data into PostgreSQL in batches
-   Marks files as processed to prevent reprocessing
-   Provides detailed logging for monitoring

## Monitoring and Logging

### Airflow UI

-   Monitor DAG runs and task status
-   View logs for each task execution
-   Check task duration and success rates

### Logs Location

-   Docker: Logs are stored in `airflow_logs` volume
-   Manual: Check `$AIRFLOW_HOME/logs`

### Key Metrics to Monitor

-   Number of files processed per run
-   Data validation success rate
-   Database insertion performance
-   Error frequency and types

## Troubleshooting

### Common Issues

1. **DAG not appearing in UI**:

    - Check that the DAG file is in the correct directory
    - Verify there are no syntax errors
    - Refresh the Airflow UI

2. **Database connection errors**:

    - Verify PostgreSQL is running
    - Check connection configuration in Airflow UI
    - Ensure database and tables exist

3. **File processing errors**:

    - Check data directory permissions
    - Verify parquet file format
    - Review task logs for specific errors

4. **Memory issues**:
    - Increase Docker memory allocation
    - Reduce batch size in data processing
    - Monitor container resource usage

### Debug Commands

```bash
# Check Airflow DAG syntax
airflow dags check fuel_exports_etl

# Test specific task
airflow tasks test fuel_exports_etl scan_for_new_files 2024-01-01

# View logs
docker-compose logs airflow-scheduler
docker-compose logs airflow-webserver
```

## Performance Considerations

### Optimization Tips

-   Adjust batch size based on available memory
-   Monitor PostgreSQL performance and tune as needed
-   Consider partitioning large tables by date
-   Use appropriate indexes for query patterns

### Scaling

-   Increase Airflow worker instances for parallel processing
-   Use external storage for large data volumes
-   Consider distributed processing for very large datasets

## Security Considerations

-   Change default passwords in production
-   Use environment variables for sensitive configuration
-   Implement proper access controls on data directories
-   Secure PostgreSQL with appropriate authentication

## Contributing

1. Fork the repository
2. Create a feature branch
3. Make changes and add tests
4. Submit a pull request

## License

This project is for educational purposes as part of a data engineering lab assignment.
